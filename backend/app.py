"""
Flask API dla FilmanScraper - Cloud-Ready Version
Endpoint do scrapowania odcinkÃ³w seriali z obsÅ‚ugÄ… cookie injection
"""

from flask import Flask, request, jsonify
from flask_cors import CORS
from scraper.filman_scraper import FilmanScraper
import os
import atexit
import threading
import json
from dotenv import load_dotenv

load_dotenv()

app = Flask(__name__)
CORS(app)

# Konfiguracja
HEADLESS_MODE = os.getenv('HEADLESS_MODE', 'True').lower() == 'true'
PROFILE_DIR = os.path.join(os.path.dirname(__file__), 'scraper', 'chrome_profile')
COOKIES_FILE = os.path.join(os.path.dirname(__file__), 'session_cookies.json')

scraper_instance: FilmanScraper = None
scraper_lock = threading.Lock()
session_cookies = None

def load_cookies_from_file():
    """Wczytuje cookies z pliku"""
    global session_cookies
    try:
        if os.path.exists(COOKIES_FILE):
            with open(COOKIES_FILE, 'r') as f:
                session_cookies = json.load(f)
                app.logger.info(f"âœ“ Loaded cookies from {COOKIES_FILE}")
                return True
    except Exception as e:
        app.logger.error(f"Error loading cookies: {e}")
    return False

def save_cookies_to_file(cookies):
    """Zapisuje cookies do pliku"""
    try:
        with open(COOKIES_FILE, 'w') as f:
            json.dump(cookies, f)
        app.logger.info(f"âœ“ Saved cookies to {COOKIES_FILE}")
        return True
    except Exception as e:
        app.logger.error(f"Error saving cookies: {e}")
        return False

def get_scraper() -> FilmanScraper:
    """Zwraca instancjÄ™ scrapera, tworzÄ…c jÄ… jeÅ›li nie istnieje (thread-safe)"""
    global scraper_instance, session_cookies
    with scraper_lock:
        if scraper_instance is None or scraper_instance.driver is None:
            if scraper_instance is not None:
                app.logger.info("ğŸ”§ Scraper istnieje ale driver jest None, tworzÄ™ nowÄ… instancjÄ™...")
            else:
                app.logger.info("ğŸ”§ Tworzenie nowej instancji FilmanScraper...")
            
            scraper_instance = FilmanScraper(
                headless=HEADLESS_MODE, 
                debug=True,
                profile_dir=PROFILE_DIR
            )
            
            # Inject cookies if available
            if session_cookies:
                app.logger.info("ğŸª Injecting stored cookies...")
                try:
                    scraper_instance.inject_cookies(session_cookies)
                    app.logger.info("âœ“ Cookies injected successfully")
                except Exception as e:
                    app.logger.error(f"Error injecting cookies: {e}")
            
            if not scraper_instance.check_if_logged_in():
                app.logger.warning("ğŸ”” Scraper nie jest zalogowany. UÅ¼yj /api/update-session aby dodaÄ‡ cookies.")
        return scraper_instance

def shutdown_scraper():
    """Zamyka scraper przy zamykaniu aplikacji"""
    global scraper_instance
    if scraper_instance:
        app.logger.info("ğŸšª Zamykanie instancji FilmanScraper...")
        scraper_instance.close()

atexit.register(shutdown_scraper)


@app.route('/api/health', methods=['GET'])
def health_check():
    """SprawdÅº czy API dziaÅ‚a i odÅ›wieÅ¼ sesjÄ™"""
    try:
        scraper = get_scraper()
        is_logged_in = scraper.check_if_logged_in() if scraper else False
        
        return jsonify({
            'status': 'ok', 
            'message': 'FilmanScraper API is running',
            'logged_in': is_logged_in,
            'has_cookies': session_cookies is not None
        })
    except Exception as e:
        return jsonify({
            'status': 'ok',
            'message': 'FilmanScraper API is running',
            'error': str(e)
        })

@app.route('/api/keep-alive', methods=['GET'])
def keep_alive():
    """Keep-alive endpoint do zapobiegania uÅ›pieniu serwera"""
    try:
        scraper = get_scraper()
        # OdÅ›wieÅ¼ sesjÄ™ odwiedzajÄ…c stronÄ™ gÅ‚Ã³wnÄ…
        if scraper and scraper.driver:
            scraper.driver.get(FilmanScraper.BASE_URL)
            is_logged_in = scraper.check_if_logged_in()
            return jsonify({
                'status': 'alive',
                'message': 'Session refreshed',
                'logged_in': is_logged_in
            })
        return jsonify({'status': 'alive', 'message': 'No active scraper'})
    except Exception as e:
        return jsonify({'status': 'alive', 'error': str(e)})

@app.route('/api/update-session', methods=['POST'])
def update_session():
    """
    Endpoint do aktualizacji cookies sesji.
    
    Body: {
        "cookies": [
            {"name": "cookie_name", "value": "cookie_value", "domain": ".filman.cc"},
            ...
        ]
    }
    lub
    Body: {
        "cookie_string": "name1=value1; name2=value2; ..."
    }
    """
    global session_cookies
    
    try:
        data = request.get_json()
        
        if not data:
            return jsonify({
                'success': False,
                'error': 'No JSON data provided'
            }), 400
        
        # ObsÅ‚uga cookie_string
        if 'cookie_string' in data:
            cookie_string = data['cookie_string']
            cookies = []
            for cookie in cookie_string.split(';'):
                cookie = cookie.strip()
                if '=' in cookie:
                    name, value = cookie.split('=', 1)
                    cookies.append({
                        'name': name.strip(),
                        'value': value.strip(),
                        'domain': '.filman.cc'
                    })
            session_cookies = cookies
        # ObsÅ‚uga listy cookies
        elif 'cookies' in data:
            session_cookies = data['cookies']
        else:
            return jsonify({
                'success': False,
                'error': 'Missing "cookies" or "cookie_string" in request body'
            }), 400
        
        # Zapisz cookies do pliku
        save_cookies_to_file(session_cookies)
        
        # JeÅ›li scraper juÅ¼ istnieje, wstrzyknij nowe cookies
        global scraper_instance
        with scraper_lock:
            if scraper_instance and scraper_instance.driver:
                app.logger.info("ğŸª Updating cookies in active scraper...")
                try:
                    scraper_instance.inject_cookies(session_cookies)
                    is_logged_in = scraper_instance.check_if_logged_in()
                    
                    return jsonify({
                        'success': True,
                        'message': 'Session cookies updated successfully',
                        'logged_in': is_logged_in,
                        'cookies_count': len(session_cookies)
                    })
                except Exception as e:
                    app.logger.error(f"Error injecting cookies: {e}")
                    return jsonify({
                        'success': False,
                        'error': f'Failed to inject cookies: {str(e)}'
                    }), 500
            else:
                # Scraper nie istnieje, tylko zapisz cookies
                return jsonify({
                    'success': True,
                    'message': 'Session cookies stored (will be used on next scraper init)',
                    'cookies_count': len(session_cookies)
                })
    
    except Exception as e:
        app.logger.error(f"Error in /api/update-session: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/test', methods=['POST'])
def test_endpoint():
    """Test endpoint bez scrapera"""
    app.logger.info("ğŸ§ª Otrzymano request na /api/test")
    data = request.get_json()
    app.logger.info(f"ğŸ“¦ Data: {data}")
    
    return jsonify({
        'success': True,
        'message': 'Test OK',
        'received': data
    })


@app.route('/api/scrape/search', methods=['POST'])
def scrape_search():
    """
    Wyszukuje serial/film i zwraca listÄ™ odcinkÃ³w.

    Body: {"title": "Breaking Bad", "type": "serial", "year": 2008}
    """
    app.logger.info(f"ğŸ” Otrzymano request /api/scrape/search")
    
    try:
        data = request.get_json()
        app.logger.info(f"ğŸ“¦ Request data: {data}")
        
        if not data or 'title' not in data:
            app.logger.error("âŒ Brak 'title' w requeÅ›cie")
            return jsonify({
                'success': False,
                'error': 'Missing title in request body'
            }), 400
        
        title = data['title']
        content_type = data.get('type', 'serial')
        year = data.get('year')
        app.logger.info(f"ğŸ¬ Szukam: '{title}', Typ: {content_type}, Rok: {year}")
        
        scraper = get_scraper()
        
        try:
            if not scraper.search_series(title):
                app.logger.error("âŒ Nie znaleziono wynikÃ³w w wyszukiwarce")
                return jsonify({
                    'success': False,
                    'error': 'Nie znaleziono wynikÃ³w'
                }), 404
        except Exception as e:
            app.logger.error(f"âŒ BÅ‚Ä…d w search_series: {e}", exc_info=True)
            return jsonify({
                'success': False,
                'error': f'BÅ‚Ä…d wyszukiwania: {str(e)}'
            }), 500
        
        search_results = scraper.get_search_results(content_type)
        
        if not search_results:
            return jsonify({
                'success': False,
                'error': 'Brak wynikÃ³w wyszukiwania'
            }), 404
        
        app.logger.info(f"ğŸ“ Znaleziono {len(search_results)} wynikÃ³w:")
        for i, result in enumerate(search_results):
            app.logger.info(f"  {i}. [{result['type']}] {result['title']} ({result['year']})")
        
        result_index = 0
        if year:
            year_str = str(year)
            app.logger.info(f"ğŸ” PrÃ³bujÄ™ dopasowaÄ‡ rok: {year_str}")
            
            for i, result in enumerate(search_results):
                if result.get('year', '') == year_str:
                    result_index = i
                    app.logger.info(f"âœ“ Znaleziono dopasowanie na pozycji {i}")
                    break
            else:
                app.logger.warning(f"âš ï¸ Nie znaleziono wyniku z rokiem {year_str}, uÅ¼ywam pierwszego z listy.")
        
        selected_result = search_results[result_index] if result_index < len(search_results) else None
        
        if not selected_result:
            return jsonify({
                'success': False,
                'error': 'Nie moÅ¼na wybraÄ‡ wyniku'
            }), 404
        
        if not scraper.select_result_by_index(result_index, content_type):
            return jsonify({
                'success': False,
                'error': 'Nie moÅ¼na otworzyÄ‡ strony z wynikami'
            }), 500
        
        episodes = scraper.extract_episodes()
        
        app.logger.info(f"âœ“ Pobrano {len(episodes)} odcinkÃ³w dla '{title}' ({selected_result['year']})")
        
        return jsonify({
            'success': True,
            'title': selected_result['title'],
            'type': selected_result['type'],
            'year': selected_result['year'],
            'url': selected_result['url'],
            'episodes': episodes,
            'count': len(episodes)
        })

    
    except Exception as e:
        app.logger.error(f"BÅ‚Ä…d w /api/scrape/search: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


@app.route('/api/scrape/links', methods=['POST'])
def scrape_links():
    """
    Pobiera linki streamingowe dla wybranych odcinkÃ³w.

    Body: {"episodes": [{"episode": "S01E01", "url": "..."}]}
    """
    app.logger.info(f"ğŸ”— Otrzymano request /api/scrape/links")

    try:
        data = request.get_json()
        
        if not data or 'episodes' not in data:
            app.logger.error("âŒ Brak 'episodes' w requeÅ›cie")
            return jsonify({
                'success': False,
                'error': 'Missing episodes in request body'
            }), 400
        
        episodes = data['episodes']
        app.logger.info(f"ğŸ”— Pobieram linki dla {len(episodes)} odcinkÃ³w.")

        scraper = get_scraper()
        
        if not scraper.is_logged_in:
            if not scraper.check_if_logged_in():
                 return jsonify({ 
                    'success': False,
                    'error': 'Not logged in. Please update session cookies via /api/update-session'
                }), 401

        results = []
        
        for ep in episodes:
            episode_url = ep.get('url')
            episode_num = ep.get('episode', 'Unknown')
            
            if not episode_url:
                continue
            
            links = scraper.extract_streaming_links(episode_url)
            
            results.append({
                'episode': episode_num,
                'url': episode_url,
                'links': links
            })
        
        app.logger.info(f"âœ“ ZakoÅ„czono pobieranie linkÃ³w dla {len(results)} odcinkÃ³w.")
        
        # NIE restartujemy scrapera - zachowujemy sesjÄ™ dla kolejnych requestÃ³w
        # try:
        #     app.logger.info("ğŸ”„ RestartujÄ™ scraper...")
        #     scraper.close()
        #     global scraper_instance
        #     with scraper_lock:
        #         scraper_instance = None
        #     app.logger.info("âœ“ Scraper zrestartowany.")
        # except Exception as e:
        #     app.logger.warning(f"âš ï¸ BÅ‚Ä…d podczas restartu scrapera: {e}")
        
        return jsonify({
            'success': True,
            'results': results,
            'count': len(results)
        })

    
    except Exception as e:
        app.logger.error(f"BÅ‚Ä…d w /api/scrape/links: {e}", exc_info=True)
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500


if __name__ == '__main__':
    # Cloud-ready: uÅ¼ywaj PORT z environment variable (Render/Heroku)
    port = int(os.getenv('PORT', os.getenv('FLASK_PORT', 5001)))
    debug = os.getenv('FLASK_DEBUG', 'False').lower() == 'true'
    
    # Wczytaj cookies z pliku przy starcie
    load_cookies_from_file()
    
    if not app.debug or os.environ.get('WERKZEUG_RUN_MAIN') == 'true':
        app.logger.info("InicjalizujÄ™ instancjÄ™ scrapera...")
        try:
            get_scraper()
        except Exception as e:
            app.logger.warning(f"Could not initialize scraper on startup: {e}")

    app.logger.info(f"ğŸš€ API startuje na porcie {port}")
    app.logger.info(f"ğŸ‘» Tryb Headless: {HEADLESS_MODE}")
    app.logger.info(f"ğŸª Cookies file: {COOKIES_FILE}")
    
    # Production: bez reloadera
    app.run(host='0.0.0.0', port=port, debug=debug, use_reloader=False)
